# yaml-language-server: $schema=https://raw.githubusercontent.com/mostlygeek/llama-swap/refs/heads/main/config-schema.json

sendLoadingState: true

macros:
  llama-server: >
    /usr/bin/llama-server
    --port ${PORT}
    --no-mmap
    -ngl 999
    --flash-attn on
    --log-disable
  
  sd-server: >
    /usr/local/bin/sd-server
    --listen-port ${PORT}
    --diffusion-fa

models:
  gpt-oss-20b:
    name: gpt-oss (20B)
    cmd: >
      ${llama-server}
      -m /models/gpt-oss-20b-F16.gguf
      -c 262144
      --reasoning-format auto
      --jinja

  gpt-oss-120b:
    name: gpt-oss (120B)
    cmd: >
      ${llama-server}
      -m /models/gpt-oss-120b-F16.gguf
      -c 262144
      --reasoning-format auto
      --jinja
  
  # TODO: Make casing consistent across all model names
  Qwen3-30B:
    name: Qwen3 (30B)
    cmd: >
      ${llama-server}
      -m /models/Qwen3-30B-A3B-Q4_K_M.gguf

  Qwen3-Coder-30B:
    name: Qwen3 Coder (30B)
    cmd: >
      ${llama-server}
      -m /models/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf
      -c 262144
      --jinja
  
  GLM-4.7-Flash:
    name: GLM-4.7 Flash
    cmd: >
      ${llama-server}
      -m /models/GLM-4.7-Flash-UD-Q8_K_XL.gguf
      -c 202752
      --min-p 0.01
      --repeat-penalty 1.0
      --temp 1.0
      --top-p 0.95
      --jinja
  
  stable-diffusion-v1-5:
    name: Stable Diffusion v1.5
    checkEndpoint: "/"
    cmd: >
      ${sd-server}
      -m /models/stable-diffusion/v1-5-pruned-emaonly.safetensors
  
  z-image-turbo:
    name: Z-Image Turbo
    checkEndpoint: "/"
    cmd: >
      ${sd-server}
      --diffusion-model /models/stable-diffusion/z_image_turbo-Q8_0.gguf
      --vae /models/stable-diffusion/ae.safetensors
      --llm /models/Qwen3-4B-Instruct-2507-Q4_K_M.gguf
      --cfg-scale 1.0
      --steps 12
  
  qwen-image-edit:
    name: Qwen Image Edit
    checkEndpoint: "/"
    cmd: >
      ${sd-server}
      --diffusion-model /models/stable-diffusion/qwen-image-edit-2511-Q5_K_M.gguf
      --vae /models/stable-diffusion/qwen_image_vae.safetensors
      --llm /models/Qwen2.5-VL-7B-Instruct.f16.gguf
      --cfg-scale 2.5
      --sampling-method euler
      --flow-shift 3
      --qwen-image-zero-cond-t
  
  flux-2-klein-4b:
    name: FLUX.2 [klein] (4B)
    checkEndpoint: "/"
    cmd: >
      ${sd-server}
      --diffusion-model /models/stable-diffusion/flux-2-klein-4b-BF16.gguf
      --vae /models/stable-diffusion/flux-2-klein-vae.safetensors
      --llm /models/Qwen3-4B-BF16.gguf
      --cfg-scale 1.0
      --steps 4
